{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-Based Link Prediction on Yelp Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices, sample\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse.linalg import inv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct full network graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertices = businesses (approximately 27K businesses)\n",
    "\n",
    "Edges = There's an edge between $i$ and $j$ if the number of users are more than xx.\n",
    "\n",
    "Note: See https://networkx.org/documentation/latest/reference/introduction.html#networkx-basics for `nx` package documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine an edge, look at the distribution of `num_users`.\n",
    "\n",
    "Couldn't plot a histogram, probably because it's highly skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.read_feather(\"data/business_edge_user_count.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>num_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0ZsqqzHu1HHkDdIKoivi5g</td>\n",
       "      <td>1An4DxtMmvvSe0HX4viRCA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ZsqqzHu1HHkDdIKoivi5g</td>\n",
       "      <td>3YqUe2FTCQr0pPVK8oCv6Q</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0ZsqqzHu1HHkDdIKoivi5g</td>\n",
       "      <td>3gXgILE2YWVidJDvVWBT6Q</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ZsqqzHu1HHkDdIKoivi5g</td>\n",
       "      <td>HpWi2CRJlxVCYKd8kS0X-A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ZsqqzHu1HHkDdIKoivi5g</td>\n",
       "      <td>KP5OncF2jhT7_J1phHPPww</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       b1                      b2  num_users\n",
       "0  0ZsqqzHu1HHkDdIKoivi5g  1An4DxtMmvvSe0HX4viRCA          4\n",
       "1  0ZsqqzHu1HHkDdIKoivi5g  3YqUe2FTCQr0pPVK8oCv6Q        105\n",
       "2  0ZsqqzHu1HHkDdIKoivi5g  3gXgILE2YWVidJDvVWBT6Q          6\n",
       "3  0ZsqqzHu1HHkDdIKoivi5g  HpWi2CRJlxVCYKd8kS0X-A          4\n",
       "4  0ZsqqzHu1HHkDdIKoivi5g  KP5OncF2jhT7_J1phHPPww         69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.065332e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.321564e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.913244e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.446000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          num_users\n",
       "count  2.065332e+07\n",
       "mean   2.321564e+00\n",
       "std    3.913244e+00\n",
       "min    1.000000e+00\n",
       "25%    1.000000e+00\n",
       "50%    1.000000e+00\n",
       "75%    2.000000e+00\n",
       "max    1.446000e+03"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(data = edge_df, x = \"num_users\")\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If cutoff = 1, there will be 20653315 edges\n",
      "If cutoff = 5, there will be 1974099 edges\n",
      "If cutoff = 10, there will be 588147 edges\n",
      "If cutoff = 20, there will be 151273 edges\n",
      "If cutoff = 50, there will be 18145 edges\n",
      "If cutoff = 100, there will be 2471 edges\n"
     ]
    }
   ],
   "source": [
    "for cutoff in [1, 5, 10, 20, 50, 100]:\n",
    "    print(f\"If cutoff = {cutoff}, there will be {len(edge_df[edge_df['num_users'] >= cutoff])} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's about 27K businesses (vertices), probably should take the cutoff = 5 for the existence of an edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "edge_subset = edge_df[edge_df['num_users'] >= cutoff]\n",
    "edge_list = list(zip(*map(edge_subset.get, ['b1', 'b2', 'num_users'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edge_list) # or use G.add_edges_from(edge_list) for unweifhted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has degree assortativity: -0.21032761437631706\n"
     ]
    }
   ],
   "source": [
    "deg_assort = nx.degree_assortativity_coefficient(G, x = 'out', y = 'out')\n",
    "print(f\"The network has degree assortativity: {deg_assort}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nx.degree_histogram(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph will be referred to as \"fully observed graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is how we approached train-test split. The process commenced with randomly dividing the existing edges (positive samples) of the graph into training and test sets, allocating 75% to the training set and 25% to the testing set.\n",
    "\n",
    "To enhance the model's predictive power, negative sampling was employed. This involved generating non-existent edges (negative samples) between random pairs of nodes. The key modification here was the efficient generation of these negative edges: instead of checking all possible node pairs, we randomly selected node pairs and verified the absence of an edge between them. The number of negative edges generated matched the number of positive edges in the test set. These negative samples were then divided using the same 75-25 split as the positive samples.\n",
    "\n",
    "The training graph was carefully constructed using only the positive edges from the training set. This step is critical to prevent data leakage and ensures that the training data reflects realistic scenarios for link prediction. The final training and test datasets were then prepared by combining their respective positive and negative edges. This balanced approach in dataset composition is essential for providing a comprehensive and unbiased evaluation of the link prediction model, thereby enhancing its applicability and accuracy in real-world recommender system scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure edge_list is a list of tuples (u, v, weight)\n",
    "edge_list = [(u, v, d['weight']) for u, v, d in G.edges(data=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting positive edges\n",
    "train_edges, test_edges = train_test_split(edge_list, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.8 s, sys: 305 ms, total: 47.1 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function to generate a single negative edge\n",
    "def generate_neg_edge(G):\n",
    "    nodes_list = list(G.nodes)  # convert nodes to a list\n",
    "    while True:\n",
    "        u, v = sample(nodes_list, 2)\n",
    "        if not G.has_edge(u, v):\n",
    "            return u, v\n",
    "\n",
    "# number of negative edges needed\n",
    "num_neg_edges_needed = len(test_edges)\n",
    "\n",
    "# generate negative edges\n",
    "neg_edges = set()\n",
    "while len(neg_edges) < num_neg_edges_needed:\n",
    "    neg_edges.add(generate_neg_edge(G))\n",
    "\n",
    "# convert to list and sample if necessary\n",
    "neg_edges_sample = list(neg_edges)\n",
    "if len(neg_edges_sample) > num_neg_edges_needed:\n",
    "    neg_edges_sample = sample(neg_edges_sample, num_neg_edges_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split negative edges into train and test\n",
    "train_neg_edges, test_neg_edges = train_test_split(neg_edges_sample, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new graph for training (useful for feature extraction)\n",
    "g_train = nx.Graph()\n",
    "g_train.add_nodes_from(G.nodes)\n",
    "g_train.add_weighted_edges_from(train_edges)  # Add only training positive edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative edges for train and test sets (what we use for actual training and testing)\n",
    "train_set = train_edges + [(u, v, 0) for u, v in train_neg_edges]  # 0 weight for negative edges\n",
    "test_set = test_edges + [(u, v, 0) for u, v in test_neg_edges]  # 0 weight for negative edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep labels - binary classification\n",
    "train_labels = [1 if w > 0 else 0 for _, _, w in train_set]\n",
    "test_labels = [1 if w > 0 else 0 for _, _, w in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity scores with different measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE global measures are really computationally intensive to calculate...\n",
    "def calculate_similarity_scores_local_and_global(G, edge_list):\n",
    "    # ========== local measures ==========\n",
    "\n",
    "    # local measures are fast to calculate (~2 min for train set)\n",
    "    \n",
    "    # precompute neighbors and degrees for each node\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # precompute Adamic-Adar contributions for each node\n",
    "    adamic_adar_contrib = {node: 1 / np.log(degrees[node]) if degrees[node] > 1 else 0 for node in G.nodes()}\n",
    "\n",
    "    # to store local similarity scores\n",
    "    common_neighbors = []\n",
    "    jaccard_coefficients = []\n",
    "    adamic_adar_indices = []\n",
    "    leicht_holme_newman = []\n",
    "    hub_promoted = []\n",
    "    hub_depressed = []\n",
    "    preferential_attachment = []\n",
    "\n",
    "    # calc local similarity measures\n",
    "    for u, v, _ in tqdm(edge_list, desc='<local>'):\n",
    "        u_neighbors = neighbors[u]\n",
    "        v_neighbors = neighbors[v]\n",
    "        common_neighbors_count = len(u_neighbors & v_neighbors)\n",
    "\n",
    "        # Common Neighbors\n",
    "        common_neighbors.append(common_neighbors_count)\n",
    "\n",
    "        # Jaccard Coefficient\n",
    "        union_size = len(u_neighbors | v_neighbors)\n",
    "        jaccard_coeff = common_neighbors_count / union_size if union_size else 0\n",
    "        jaccard_coefficients.append(jaccard_coeff)\n",
    "\n",
    "        # Adamic-Adar Index\n",
    "        adamic_adar_index = sum(adamic_adar_contrib[w] for w in u_neighbors & v_neighbors)\n",
    "        adamic_adar_indices.append(adamic_adar_index)\n",
    "\n",
    "        # Leicht-Holme-Newman Score\n",
    "        lhn_index = common_neighbors_count / (degrees[u] * degrees[v]) if degrees[u] * degrees[v] else 0\n",
    "        leicht_holme_newman.append(lhn_index)\n",
    "\n",
    "        # Hub Promoted Index\n",
    "        min_degree = min(degrees[u], degrees[v])\n",
    "        hpi_index = common_neighbors_count / min_degree if min_degree else 0\n",
    "        hub_promoted.append(hpi_index)\n",
    "\n",
    "        # Hub Depressed Index\n",
    "        max_degree = max(degrees[u], degrees[v])\n",
    "        hdi_index = common_neighbors_count / max_degree if max_degree else 0\n",
    "        hub_depressed.append(hdi_index)\n",
    "\n",
    "        # Preferential Attachment Score\n",
    "        pa_score = degrees[u] * degrees[v]\n",
    "        preferential_attachment.append(pa_score)\n",
    "\n",
    "    # ========== global measures ==========\n",
    "\n",
    "    # global similarity scores (Katz, Average Commute Time, PageRank, SimRank)\n",
    "    # note: these can be computationally intensive\n",
    "\n",
    "    # init global similarity scores\n",
    "    katz_indices = {}\n",
    "    average_commute_times = {}\n",
    "    pageranks = nx.pagerank(G)  # PageRank for the entire graph\n",
    "    simranks = nx.simrank_similarity(G)  # SimRank for the entire graph\n",
    "\n",
    "    # Katz Index\n",
    "    beta = 0.005  # NOTE beta can be adjusted\n",
    "    identity_matrix = np.eye(len(G))\n",
    "    adjacency_matrix = nx.to_numpy_array(G)\n",
    "    katz_matrix = inv(identity_matrix - beta * adjacency_matrix) - identity_matrix\n",
    "\n",
    "    # average commute time (random walk)\n",
    "    # note: this can be very computationally intensive\n",
    "\n",
    "    # Convert the Laplacian matrix to CSC format\n",
    "    laplacian_matrix = nx.laplacian_matrix(G)\n",
    "    laplacian_csc = csc_matrix(laplacian_matrix, dtype=float)\n",
    "    \n",
    "    # Compute the Moore-Penrose pseudoinverse of the Laplacian matrix\n",
    "    pseudoinverse_laplacian = pinv(laplacian_csc.toarray())\n",
    "    \n",
    "    # Calculate Average Commute Time\n",
    "    average_commute_times = {}\n",
    "    for u, v, _ in tqdm(edge_list):\n",
    "        u_idx, v_idx = list(G.nodes()).index(u), list(G.nodes()).index(v)\n",
    "        avg_commute_time = pseudoinverse_laplacian[u_idx, u_idx] + pseudoinverse_laplacian[v_idx, v_idx] - 2 * pseudoinverse_laplacian[u_idx, v_idx]\n",
    "        average_commute_times[(u, v)] = avg_commute_time\n",
    "\n",
    "    # Katz\n",
    "    for u, v, _ in tqdm(edge_list, desc='<global: katz>'):\n",
    "        u_idx, v_idx = list(G.nodes()).index(u), list(G.nodes()).index(v)\n",
    "        \n",
    "        # Katz Index\n",
    "        katz_indices[(u, v)] = katz_matrix[u_idx, v_idx]\n",
    "\n",
    "    return {\n",
    "        # local measures\n",
    "        'common_neighbors': common_neighbors,\n",
    "        'jaccard_coefficients': jaccard_coefficients,\n",
    "        'adamic_adar_indices': adamic_adar_indices,\n",
    "        'leicht_holme_newman': leicht_holme_newman,\n",
    "        'hub_promoted': hub_promoted,\n",
    "        'hub_depressed': hub_depressed,\n",
    "        'preferential_attachment': preferential_attachment,\n",
    "        # global measures\n",
    "        'katz_indices': katz_indices,\n",
    "        'average_commute_times': average_commute_times,\n",
    "        'pageranks': pageranks,\n",
    "        'simranks': simranks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE just calculate local measures instead\n",
    "def calculate_similarity_scores_local(G, edge_list):\n",
    "    # local measures are fast to calculate (~2 min for train set)\n",
    "    \n",
    "    # precompute neighbors and degrees for each node\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # precompute Adamic-Adar contributions for each node\n",
    "    adamic_adar_contrib = {node: 1 / np.log(degrees[node]) if degrees[node] > 1 else 0 for node in G.nodes()}\n",
    "\n",
    "    # to store local similarity scores\n",
    "    common_neighbors = []\n",
    "    jaccard_coefficients = []\n",
    "    adamic_adar_indices = []\n",
    "    leicht_holme_newman = []\n",
    "    hub_promoted = []\n",
    "    hub_depressed = []\n",
    "    preferential_attachment = []\n",
    "\n",
    "    # calc local similarity measures\n",
    "    for u, v, _ in tqdm(edge_list, desc='<local>'):\n",
    "        u_neighbors = neighbors[u]\n",
    "        v_neighbors = neighbors[v]\n",
    "        common_neighbors_count = len(u_neighbors & v_neighbors)\n",
    "\n",
    "        # Common Neighbors\n",
    "        common_neighbors.append(common_neighbors_count)\n",
    "\n",
    "        # Jaccard Coefficient\n",
    "        union_size = len(u_neighbors | v_neighbors)\n",
    "        jaccard_coeff = common_neighbors_count / union_size if union_size else 0\n",
    "        jaccard_coefficients.append(jaccard_coeff)\n",
    "\n",
    "        # Adamic-Adar Index\n",
    "        adamic_adar_index = sum(adamic_adar_contrib[w] for w in u_neighbors & v_neighbors)\n",
    "        adamic_adar_indices.append(adamic_adar_index)\n",
    "\n",
    "        # Leicht-Holme-Newman Score\n",
    "        lhn_index = common_neighbors_count / (degrees[u] * degrees[v]) if degrees[u] * degrees[v] else 0\n",
    "        leicht_holme_newman.append(lhn_index)\n",
    "\n",
    "        # Hub Promoted Index\n",
    "        min_degree = min(degrees[u], degrees[v])\n",
    "        hpi_index = common_neighbors_count / min_degree if min_degree else 0\n",
    "        hub_promoted.append(hpi_index)\n",
    "\n",
    "        # Hub Depressed Index\n",
    "        max_degree = max(degrees[u], degrees[v])\n",
    "        hdi_index = common_neighbors_count / max_degree if max_degree else 0\n",
    "        hub_depressed.append(hdi_index)\n",
    "\n",
    "        # Preferential Attachment Score\n",
    "        pa_score = degrees[u] * degrees[v]\n",
    "        preferential_attachment.append(pa_score)\n",
    "\n",
    "    return {\n",
    "        'common_neighbors': common_neighbors,\n",
    "        'jaccard_coefficients': jaccard_coefficients,\n",
    "        'adamic_adar_indices': adamic_adar_indices,\n",
    "        'leicht_holme_newman': leicht_holme_newman,\n",
    "        'hub_promoted': hub_promoted,\n",
    "        'hub_depressed': hub_depressed,\n",
    "        'preferential_attachment': preferential_attachment,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<local>: 100%|███████████████████████████████| 1850717/1850717 [01:56<00:00, 15838.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 1.49 s, total: 1min 56s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim_scores_train = calculate_similarity_scores_local(g_train, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<local>: 100%|█████████████████████████████████| 616907/616907 [00:39<00:00, 15618.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 823 ms, total: 39.8 s\n",
      "Wall time: 40.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim_scores_test = calculate_similarity_scores_local(g_train, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification with link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to predict whether a link should exist between two nodes (AKA how confident... binary classification). Each model is an XGBoost and uses a single similarity measure as its feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train and evaluate a model\n",
    "def train_evaluate_model(train_features, train_labels, test_features, test_labels):\n",
    "    # training the model\n",
    "    model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "    model.fit(train_features, train_labels)\n",
    "\n",
    "    # predicting on the test set\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # evaluating the model\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    roc_auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 7/7 [02:26<00:00, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 18s, sys: 3.65 s, total: 5min 22s\n",
      "Wall time: 2min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training and evaluating models for each similarity measure\n",
    "results = []\n",
    "for feature_name in tqdm(sim_scores_train):\n",
    "    train_feature = sim_scores_train[feature_name]\n",
    "    test_feature = sim_scores_test[feature_name]\n",
    "    \n",
    "    curr_eval = train_evaluate_model(\n",
    "        np.array(train_feature).reshape(-1, 1), train_labels,\n",
    "        np.array(test_feature).reshape(-1, 1), test_labels\n",
    "    )\n",
    "    results.append([feature_name] + list(curr_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(results, columns=['similarity_measure', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res\\\n",
    "    .sort_values('roc_auc', ascending=False)\\\n",
    "    .round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_measure</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adamic_adar_indices</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hub_promoted</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_neighbors</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jaccard_coefficients</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hub_depressed</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leicht_holme_newman</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>preferential_attachment</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity_measure  accuracy  precision  recall     f1  roc_auc\n",
       "2      adamic_adar_indices     0.971      0.975   0.990  0.982    0.943\n",
       "4             hub_promoted     0.970      0.973   0.990  0.981    0.941\n",
       "0         common_neighbors     0.968      0.972   0.988  0.980    0.937\n",
       "1     jaccard_coefficients     0.957      0.957   0.991  0.974    0.906\n",
       "5            hub_depressed     0.955      0.956   0.989  0.972    0.904\n",
       "3      leicht_holme_newman     0.956      0.952   0.995  0.973    0.898\n",
       "6  preferential_attachment     0.924      0.940   0.967  0.953    0.859"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('res_local.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to run entire notebook: 0:06:02.202218\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "print(f'Time elapsed to run entire notebook: {t1 - t0}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
